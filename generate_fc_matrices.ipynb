{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset\n",
    "###--- import packages\n",
    "import nilearn, scipy, os, bct\n",
    "from nilearn import input_data, signal, plotting\n",
    "from nilearn.input_data import NiftiLabelsMasker\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###--- Enter BIDS directory.\n",
    "MainDir = '/home/connectomics/LearningBrain'\n",
    "\n",
    "###--- Import task-data (resting state and dual n-back) \".nii\" files and confounds - save list of locations.\n",
    "rest_filenames = []\n",
    "dual_filenames = []\n",
    "rest_confounds = []\n",
    "dual_confounds = []\n",
    "for dirpath, dirnames, filenames in os.walk(MainDir):\n",
    "    for filename in filenames:\n",
    "        if ('rest_bold_space-MNI152NLin2009cAsym_variant-smoothAROMAnonaggr_preproc.nii.gz' in filename) and (MainDir)+'/derivatives/fmriprep/sub' in os.path.join(dirpath, filename):\n",
    "            rest_filenames.append(os.path.join(dirpath, filename))\n",
    "        if ('dualnback_bold_space-MNI152NLin2009cAsym_variant-smoothAROMAnonaggr_preproc.nii.gz' in filename) and (MainDir)+'/derivatives/fmriprep/sub'  in os.path.join(dirpath, filename):\n",
    "            dual_filenames.append(os.path.join(dirpath, filename))\n",
    "        if ('rest_bold_confounds.tsv' in filename) and (MainDir)+'/derivatives/fmriprep/sub' in os.path.join(dirpath, filename):\n",
    "            rest_confounds.append(os.path.join(dirpath, filename))\n",
    "        if ('dualnback_bold_confounds.tsv' in filename) and (MainDir)+'/derivatives/fmriprep/sub' in os.path.join(dirpath, filename):\n",
    "            dual_confounds.append(os.path.join(dirpath, filename))\n",
    "dual_confounds.sort()\n",
    "dual_filenames.sort()\n",
    "rest_confounds.sort()\n",
    "rest_filenames.sort()\n",
    "print('number of rest connectivity files is equal ' +str(len(rest_filenames)))\n",
    "print('number of rest confound files is equal ' +str(len(rest_confounds)))\n",
    "print('number of dual connectivity files is equal ' +str(len(dual_filenames)))\n",
    "print('number of dual confound files is equal ' +str(len(dual_confounds)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######--- HYPERPARAMETERS ---#######\n",
    "###--- Connectivity measures\n",
    "models = ['correlation', 'partial correlation', 'covariance', 'precision']\n",
    "\n",
    "###--- Parcellations\n",
    "power = nilearn.datasets.fetch_coords_power_2011()\n",
    "dosenbach = nilearn.datasets.fetch_coords_dosenbach_2010()\n",
    "multiscale = nilearn.datasets.fetch_atlas_basc_multiscale_2015()\n",
    "aal = nilearn.datasets.fetch_atlas_aal()\n",
    "harvard = nilearn.datasets.fetch_atlas_harvard_oxford('cort-maxprob-thr0-1mm',symmetric_split=True)\n",
    "#--- Create atlases as spheres from Power and Dosenbach coordinates\n",
    "coords_power = np.vstack((power.rois['x'], power.rois['y'], power.rois['z'])).T\n",
    "pow_masker = nilearn.input_data.NiftiSpheresMasker(seeds=coords_power, radius=3, standardize=True, detrend=True)\n",
    "coords_dosenbach = np.vstack((dosenbach.rois['x'], dosenbach.rois['y'], dosenbach.rois['z'])).T\n",
    "dos_masker = nilearn.input_data.NiftiSpheresMasker(seeds=coords_dosenbach, radius=3, standardize=True, detrend=True)\n",
    "#--- Load Harvard, AAL, Multiscale atlases as masks\n",
    "har_masker = nilearn.input_data.NiftiLabelsMasker(labels_img = harvard.maps, standardize=True, detrend=True)\n",
    "aal_masker = nilearn.input_data.NiftiLabelsMasker(labels_img = aal.maps, standardize=True, detrend=True)\n",
    "mul_masker = nilearn.input_data.NiftiLabelsMasker(labels_img = multiscale.scale197, standardize=True, detrend=True)\n",
    "#--- Final list of maskers to iterate over.\n",
    "maskers = [pow_masker, dos_masker, mul_masker, aal_masker, har_masker]\n",
    "masker_names=['pow','dos','mul','aal','har']\n",
    "print('Atlas preparation complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###--- Choose the data to work on (only resting-state)\n",
    "func_filenames = rest_filenames\n",
    "confounds = rest_confounds\n",
    "\n",
    "###--- Denoising\n",
    "#--- choose only the important regressors (CSF, White Matter, Linear Trend) and save new confounds file\n",
    "for file,confound in zip(func_filenames,confounds):\n",
    "    conf=pd.read_csv(confound,delimiter='\\t')\n",
    "    conf=conf[conf.filter(regex='WhiteMatter|CSF|CompCor|X|Y|Z').columns]\n",
    "    np.savetxt(confound[:-4]+'_edited.csv',conf,delimiter=',')\n",
    "print('Choice of important regressors complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###--- Iterate over all hyperparameters and generate a functional connectivity matrix for each set of parameters.\n",
    "for file,confound in zip(func_filenames,confounds):\n",
    "    for model in models: #models:\n",
    "        for masker, masker_name in zip(maskers, masker_names):\n",
    "            #--- Create time-series in all ROIs and apply filters\n",
    "            time_series_power = masker.fit_transform(file, confounds=confound[:-4]+'_edited.csv')\n",
    "            time_series_power_filtered=signal.clean(time_series_power,low_pass=0.08,high_pass=0.009)\n",
    "            #--- Create the connectivity matrices\n",
    "            correlation_measure = ConnectivityMeasure(kind = model)\n",
    "            correlation_matrix_raw = correlation_measure.fit_transform([time_series_power_filtered])[0]\n",
    "            #--- Mask the main diagonal:\n",
    "            np.fill_diagonal(correlation_matrix_raw, 0)\n",
    "            #--- Transform matrix into vector and apply z-score\n",
    "            indices = np.triu_indices_from(correlation_matrix_raw,k=1)\n",
    "            correlation_vec = np.asarray(correlation_matrix_raw[indices])\n",
    "            zscore = scipy.stats.zscore(correlation_vec)\n",
    "            correlation_matrix = np.zeros((len(correlation_matrix_raw),len(correlation_matrix_raw)))      \n",
    "            correlation_matrix[indices]=correlation_vec\n",
    "            correlation_matrix = correlation_matrix + correlation_matrix.T\n",
    "            ###--- SAVE RESULTS \n",
    "            #--- extract sub number from file name \n",
    "            sub = int(file[file.find('sub-')+4:file.find('sub-')+6])\n",
    "            #--- extract session number from file name\n",
    "            sess = int(file[file.find('ses-')+4:file.find('ses-')+5])\n",
    "            # plotting matrices to files\n",
    "            display = plotting.plot_matrix(correlation_matrix, vmin=-0.8, vmax=0.8, colorbar=True)    \n",
    "            display.figure.savefig('matKFLB_sub'+'{:02d}'.format(sub)+ '_ses' + str(sess) + '_' + masker_name[:3] + '_' + model[:3] + '.png')     \n",
    "            plt.clf()\n",
    "            # plotting histograms of data\n",
    "            plt.hist(np.ravel(correlation_matrix), bins=100)\n",
    "            plt.savefig('matKFLB_sub'+'{:02d}'.format(sub)+ '_ses' + str(sess) + '_' + masker_name[:3] + '_' + model[:3] + 'histogram.png')\n",
    "            plt.clf()\n",
    "            #--- save connectivity matrix as human-readable data (TXT)\n",
    "            np.savetxt('matKFLB_sub'+'{:02d}'.format(sub)+ '_ses' + str(sess) + '_' + masker_name[:3] + '_' + model[:3] + '.txt', correlation_matrix)\n",
    "            print('Subject ' + str(sub) + ', session ' + str(sess) + ', model: '+ str(model) +' and atlas: '+str(masker_name)+' complete.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
